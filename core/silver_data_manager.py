"""
ç™½é“¶ç›¸å…³æ€§åˆ†æ - æ•°æ®ç®¡ç†å™¨

è´Ÿè´£ä»MT5æ‹‰å–æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°ï¼Œæ”¯æŒæ•°æ®ç¼“å­˜å’Œå¢é‡æ›´æ–°
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
import json
import os
from pathlib import Path
import pickle
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ metatrader_tools
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥MT5å®¢æˆ·ç«¯
from metatrader_tools.mt5_client.client import MT5Client, MT5Credentials
from metatrader_tools.mt5_client.periods import timeframe_from_str

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataManager:
    """æ•°æ®ç®¡ç†å™¨ - è´Ÿè´£æ•°æ®çš„è·å–ã€ä¿å­˜å’ŒåŠ è½½"""
    
    def __init__(self, data_dir: str = "market_data"):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.data_dir / "raw_data").mkdir(exist_ok=True)
        (self.data_dir / "processed_data").mkdir(exist_ok=True)
        (self.data_dir / "cache").mkdir(exist_ok=True)
        
        self.mt5_client = None
        
    def connect_mt5(self) -> bool:
        """è¿æ¥MT5"""
        try:
            if self.mt5_client is None:
                self.mt5_client = MT5Client()
                self.mt5_client.initialize()
            return True
        except Exception as e:
            logger.error(f"MT5è¿æ¥å¤±è´¥: {e}")
            return False
    
    def disconnect_mt5(self):
        """æ–­å¼€MT5è¿æ¥"""
        if self.mt5_client:
            try:
                self.mt5_client.shutdown()
                self.mt5_client = None
            except Exception as e:
                logger.error(f"æ–­å¼€MT5è¿æ¥å¤±è´¥: {e}")
    
    def get_data_filename(self, symbol: str, timeframe: str) -> str:
        """ç”Ÿæˆæ•°æ®æ–‡ä»¶å"""
        return f"{symbol}_{timeframe}.csv"
    
    def get_cache_filename(self, symbol: str, timeframe: str) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        return f"{symbol}_{timeframe}_cache.pkl"
    
    def fetch_from_mt5(self, symbol: str, timeframe: str, count: int = 5000) -> Optional[pd.DataFrame]:
        """ä»MT5è·å–æ•°æ®"""
        try:
            if not self.connect_mt5():
                return None
            
            logger.info(f"ä»MT5è·å– {symbol} {timeframe} æ•°æ®ï¼Œæ•°é‡: {count}")
            
            tf_const = timeframe_from_str(timeframe)
            data = self.mt5_client.get_rates(symbol, tf_const, count=count)
            
            if data.empty:
                logger.warning(f"æœªè·å–åˆ° {symbol} {timeframe} çš„æ•°æ®")
                return None
            
            logger.info(f"æˆåŠŸè·å– {symbol} {timeframe} æ•°æ®: {len(data)} æ ¹Kçº¿")
            logger.info(f"æ—¶é—´èŒƒå›´: {data.index.min()} åˆ° {data.index.max()}")
            
            return data
            
        except Exception as e:
            logger.error(f"ä»MT5è·å– {symbol} {timeframe} æ•°æ®å¤±è´¥: {e}")
            return None
    
    def save_data_to_csv(self, data: pd.DataFrame, symbol: str, timeframe: str) -> bool:
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        try:
            filename = self.data_dir / "raw_data" / self.get_data_filename(symbol, timeframe)
            
            # æ·»åŠ å…ƒæ•°æ®
            data_with_meta = data.copy()
            data_with_meta.attrs = {
                'symbol': symbol,
                'timeframe': timeframe,
                'last_update': datetime.now().isoformat(),
                'total_bars': len(data)
            }
            
            # ä¿å­˜åˆ°CSV
            data_with_meta.to_csv(filename)
            
            # ä¿å­˜å…ƒæ•°æ®åˆ°JSON
            meta_filename = filename.with_suffix('.json')
            with open(meta_filename, 'w', encoding='utf-8') as f:
                json.dump(data_with_meta.attrs, f, indent=2, ensure_ascii=False)
            
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def load_data_from_csv(self, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            filename = self.data_dir / "raw_data" / self.get_data_filename(symbol, timeframe)
            
            if not filename.exists():
                logger.info(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                return None
            
            # åŠ è½½æ•°æ®
            data = pd.read_csv(filename, index_col=0, parse_dates=True)
            
            # åŠ è½½å…ƒæ•°æ®
            meta_filename = filename.with_suffix('.json')
            if meta_filename.exists():
                with open(meta_filename, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    data.attrs = metadata
                    logger.info(f"åŠ è½½æ•°æ®: {symbol} {timeframe}, æœ€åæ›´æ–°: {metadata.get('last_update', 'N/A')}")
            
            return data
            
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return None
    
    def is_data_fresh(self, symbol: str, timeframe: str, max_age_hours: int = 1) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦æ–°é²œ"""
        try:
            filename = self.data_dir / "raw_data" / self.get_data_filename(symbol, timeframe)
            meta_filename = filename.with_suffix('.json')
            
            if not meta_filename.exists():
                return False
            
            with open(meta_filename, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            last_update_str = metadata.get('last_update')
            if not last_update_str:
                return False
            
            last_update = datetime.fromisoformat(last_update_str)
            age_hours = (datetime.now() - last_update).total_seconds() / 3600
            
            is_fresh = age_hours < max_age_hours
            logger.info(f"{symbol} {timeframe} æ•°æ®å¹´é¾„: {age_hours:.1f}å°æ—¶, æ˜¯å¦æ–°é²œ: {is_fresh}")
            
            return is_fresh
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®æ–°é²œåº¦å¤±è´¥: {e}")
            return False
    
    def get_data(self, symbol: str, timeframe: str, count: int = 5000, 
                 force_refresh: bool = False, max_age_hours: int = 1) -> Optional[pd.DataFrame]:
        """
        è·å–æ•°æ® - ä¼˜å…ˆä»æœ¬åœ°åŠ è½½ï¼Œå¿…è¦æ—¶ä»MT5æ›´æ–°
        
        Args:
            symbol: å“ç§ä»£ç 
            timeframe: æ—¶é—´æ¡†æ¶
            count: è·å–çš„Kçº¿æ•°é‡ï¼ˆæœ€å°éœ€è¦ï¼‰
            force_refresh: å¼ºåˆ¶åˆ·æ–°æ•°æ®
            max_age_hours: æ•°æ®æœ€å¤§å¹´é¾„ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            æ•°æ®DataFrameæˆ–None
        """
        # å¦‚æœå¼ºåˆ¶åˆ·æ–°æˆ–æ•°æ®ä¸æ–°é²œï¼Œä»MT5è·å–
        if force_refresh or not self.is_data_fresh(symbol, timeframe, max_age_hours):
            logger.info(f"éœ€è¦æ›´æ–° {symbol} {timeframe} æ•°æ®")
            
            # ä»MT5è·å–æ•°æ®
            data = self.fetch_from_mt5(symbol, timeframe, count)
            
            if data is not None:
                # ä¿å­˜åˆ°æœ¬åœ°
                self.save_data_to_csv(data, symbol, timeframe)
                return data
            else:
                # å¦‚æœMT5è·å–å¤±è´¥ï¼Œå°è¯•åŠ è½½æœ¬åœ°æ•°æ®
                logger.warning(f"MT5è·å–å¤±è´¥ï¼Œå°è¯•åŠ è½½æœ¬åœ°æ•°æ®")
                return self.load_data_from_csv(symbol, timeframe)
        else:
            # æ•°æ®æ–°é²œï¼Œç›´æ¥ä»æœ¬åœ°åŠ è½½
            logger.info(f"ä»æœ¬åœ°åŠ è½½ {symbol} {timeframe} æ•°æ®")
            data = self.load_data_from_csv(symbol, timeframe)
            
            # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
            if data is not None and len(data) < count:
                logger.warning(f"{symbol} {timeframe} æœ¬åœ°æ•°æ®åªæœ‰ {len(data)} æ ¹ï¼Œå°‘äºéœ€è¦çš„ {count} æ ¹")
                logger.info(f"å°è¯•ä»MT5è·å–æ›´å¤šæ•°æ®...")
                
                # å°è¯•ä»MT5è·å–æ›´å¤šæ•°æ®
                new_data = self.fetch_from_mt5(symbol, timeframe, count)
                if new_data is not None and len(new_data) > len(data):
                    logger.info(f"æˆåŠŸè·å– {len(new_data)} æ ¹Kçº¿")
                    self.save_data_to_csv(new_data, symbol, timeframe)
                    return new_data
                else:
                    logger.warning(f"æ— æ³•è·å–æ›´å¤šæ•°æ®ï¼Œä½¿ç”¨ç°æœ‰çš„ {len(data)} æ ¹Kçº¿")
            
            return data
    
    def batch_update_data(self, symbols_config: Dict[str, List[str]], count: int = 5000) -> Dict[str, Dict[str, bool]]:
        """
        æ‰¹é‡æ›´æ–°æ•°æ®
        
        Args:
            symbols_config: å“ç§é…ç½® {symbol: [timeframes]}
            count: è·å–çš„Kçº¿æ•°é‡
            
        Returns:
            æ›´æ–°ç»“æœ {symbol: {timeframe: success}}
        """
        results = {}
        
        logger.info(f"å¼€å§‹æ‰¹é‡æ›´æ–°æ•°æ®...")
        
        for symbol, timeframes in symbols_config.items():
            results[symbol] = {}
            
            for timeframe in timeframes:
                try:
                    logger.info(f"æ›´æ–° {symbol} {timeframe}...")
                    
                    data = self.fetch_from_mt5(symbol, timeframe, count)
                    
                    if data is not None:
                        success = self.save_data_to_csv(data, symbol, timeframe)
                        results[symbol][timeframe] = success
                    else:
                        results[symbol][timeframe] = False
                        
                except Exception as e:
                    logger.error(f"æ›´æ–° {symbol} {timeframe} å¤±è´¥: {e}")
                    results[symbol][timeframe] = False
        
        # ç»Ÿè®¡ç»“æœ
        total_tasks = sum(len(timeframes) for timeframes in symbols_config.values())
        successful_tasks = sum(
            sum(1 for success in symbol_results.values() if success)
            for symbol_results in results.values()
        )
        
        logger.info(f"æ‰¹é‡æ›´æ–°å®Œæˆ: {successful_tasks}/{total_tasks} æˆåŠŸ")
        
        return results
    
    def get_data_summary(self) -> Dict[str, any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        summary = {
            'total_files': 0,
            'symbols': set(),
            'timeframes': set(),
            'data_details': []
        }
        
        raw_data_dir = self.data_dir / "raw_data"
        
        for csv_file in raw_data_dir.glob("*.csv"):
            try:
                # è§£ææ–‡ä»¶å
                name_parts = csv_file.stem.split('_')
                if len(name_parts) >= 2:
                    symbol = name_parts[0]
                    timeframe = '_'.join(name_parts[1:])
                    
                    summary['symbols'].add(symbol)
                    summary['timeframes'].add(timeframe)
                    summary['total_files'] += 1
                    
                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    meta_file = csv_file.with_suffix('.json')
                    if meta_file.exists():
                        with open(meta_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        
                        summary['data_details'].append({
                            'symbol': symbol,
                            'timeframe': timeframe,
                            'last_update': metadata.get('last_update', 'N/A'),
                            'total_bars': metadata.get('total_bars', 0),
                            'file_size': csv_file.stat().st_size
                        })
                        
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {e}")
        
        summary['symbols'] = sorted(list(summary['symbols']))
        summary['timeframes'] = sorted(list(summary['timeframes']))
        
        return summary
    
    def print_data_summary(self):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        summary = self.get_data_summary()
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æœ¬åœ°æ•°æ®æ‘˜è¦")
        print(f"{'='*60}")
        print(f"æ•°æ®æ–‡ä»¶æ€»æ•°: {summary['total_files']}")
        print(f"å“ç§æ•°é‡: {len(summary['symbols'])}")
        print(f"æ—¶é—´æ¡†æ¶æ•°é‡: {len(summary['timeframes'])}")
        
        print(f"\nğŸ“ˆ å“ç§åˆ—è¡¨: {', '.join(summary['symbols'])}")
        print(f"â° æ—¶é—´æ¡†æ¶: {', '.join(summary['timeframes'])}")
        
        print(f"\nğŸ“‹ è¯¦ç»†ä¿¡æ¯:")
        print("-" * 80)
        print(f"{'å“ç§':<10} {'æ—¶é—´æ¡†æ¶':<8} {'æœ€åæ›´æ–°':<20} {'Kçº¿æ•°':<8} {'æ–‡ä»¶å¤§å°'}")
        print("-" * 80)
        
        for detail in summary['data_details']:
            last_update = detail['last_update']
            if last_update != 'N/A':
                try:
                    update_time = datetime.fromisoformat(last_update)
                    last_update = update_time.strftime('%m-%d %H:%M')
                except:
                    pass
            
            file_size = detail['file_size']
            if file_size > 1024 * 1024:
                size_str = f"{file_size / (1024*1024):.1f}MB"
            elif file_size > 1024:
                size_str = f"{file_size / 1024:.1f}KB"
            else:
                size_str = f"{file_size}B"
            
            print(f"{detail['symbol']:<10} {detail['timeframe']:<8} "
                  f"{last_update:<20} {detail['total_bars']:<8} {size_str}")
    
    def clean_old_data(self, days_old: int = 7):
        """æ¸…ç†æ—§æ•°æ®"""
        try:
            cutoff_time = datetime.now() - timedelta(days=days_old)
            cleaned_count = 0
            
            raw_data_dir = self.data_dir / "raw_data"
            
            for csv_file in raw_data_dir.glob("*.csv"):
                meta_file = csv_file.with_suffix('.json')
                
                if meta_file.exists():
                    try:
                        with open(meta_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        
                        last_update_str = metadata.get('last_update')
                        if last_update_str:
                            last_update = datetime.fromisoformat(last_update_str)
                            
                            if last_update < cutoff_time:
                                csv_file.unlink()
                                meta_file.unlink()
                                cleaned_count += 1
                                logger.info(f"åˆ é™¤æ—§æ•°æ®: {csv_file.name}")
                                
                    except Exception as e:
                        logger.error(f"å¤„ç†æ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {e}")
            
            logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ—§æ•°æ®æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        self.disconnect_mt5()


def main():
    """ä¸»å‡½æ•° - æ•°æ®ç®¡ç†å·¥å…·"""
    print("ğŸ“Š ç™½é“¶ç›¸å…³æ€§åˆ†æ - æ•°æ®ç®¡ç†å™¨")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
    data_manager = DataManager()
    
    # é…ç½®è¦è·å–çš„æ•°æ® - æ ¹æ®MT5ç»çºªå•†æ”¯æŒçš„ä»£ç 
    symbols_config = {
        'XAGUSD': ['H4'],  # ç™½é“¶4å°æ—¶ - æ£€æµ‹æ ‡çš„
        'XAUUSD': ['H1', 'H4'],  # é»„é‡‘
        'XTIUSD': ['H1', 'H4'],  # WTIåŸæ²¹
        'XBRUSD': ['H1', 'H4'],  # å¸ƒä¼¦ç‰¹åŸæ²¹
        'US500': ['H1', 'H4'],   # æ ‡æ™®500
        'US30': ['H1', 'H4'],    # é“ç¼æ–¯
        'NAS100': ['H1', 'H4'],  # çº³æ–¯è¾¾å…‹100
        'EURUSD': ['H1', 'H4'],  # æ¬§å…ƒç¾å…ƒ
        'GBPUSD': ['H1', 'H4'],  # è‹±é•‘ç¾å…ƒ
    }
    
    try:
        while True:
            print(f"\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. æ‰¹é‡æ›´æ–°æ‰€æœ‰æ•°æ®")
            print("2. æ›´æ–°å•ä¸ªå“ç§æ•°æ®")
            print("3. æŸ¥çœ‹æ•°æ®æ‘˜è¦")
            print("4. æ¸…ç†æ—§æ•°æ®")
            print("5. æµ‹è¯•æ•°æ®è·å–")
            print("6. é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-6): ").strip()
            
            if choice == '1':
                print("\nğŸ”„ å¼€å§‹æ‰¹é‡æ›´æ–°æ•°æ®...")
                results = data_manager.batch_update_data(symbols_config, count=5000)
                
                print(f"\nğŸ“Š æ›´æ–°ç»“æœ:")
                for symbol, symbol_results in results.items():
                    for timeframe, success in symbol_results.items():
                        status = "âœ…" if success else "âŒ"
                        print(f"{status} {symbol} {timeframe}")
                
            elif choice == '2':
                symbol = input("è¯·è¾“å…¥å“ç§ä»£ç  (å¦‚ XAGUSD): ").strip().upper()
                timeframe = input("è¯·è¾“å…¥æ—¶é—´æ¡†æ¶ (å¦‚ H1): ").strip().upper()
                
                if symbol and timeframe:
                    print(f"\nğŸ”„ æ›´æ–° {symbol} {timeframe} æ•°æ®...")
                    data = data_manager.get_data(symbol, timeframe, force_refresh=True)
                    
                    if data is not None:
                        print(f"âœ… æ›´æ–°æˆåŠŸ: {len(data)} æ ¹Kçº¿")
                        print(f"æ—¶é—´èŒƒå›´: {data.index.min()} åˆ° {data.index.max()}")
                    else:
                        print("âŒ æ›´æ–°å¤±è´¥")
                
            elif choice == '3':
                data_manager.print_data_summary()
                
            elif choice == '4':
                days = input("è¯·è¾“å…¥æ¸…ç†å¤©æ•° (é»˜è®¤7å¤©): ").strip()
                try:
                    days = int(days) if days else 7
                    print(f"\nğŸ§¹ æ¸…ç† {days} å¤©å‰çš„æ•°æ®...")
                    data_manager.clean_old_data(days)
                    print("âœ… æ¸…ç†å®Œæˆ")
                except ValueError:
                    print("âŒ æ— æ•ˆçš„å¤©æ•°")
                
            elif choice == '5':
                print("\nğŸ”§ æµ‹è¯•æ•°æ®è·å–...")
                test_data = data_manager.get_data('XAGUSD', 'H4', count=50)
                
                if test_data is not None:
                    print(f"âœ… æµ‹è¯•æˆåŠŸ")
                    print(f"æ•°æ®é‡: {len(test_data)} æ ¹Kçº¿")
                    print(f"æœ€æ–°ä»·æ ¼: {test_data['close'].iloc[-1]:.4f}")
                    print(f"æ—¶é—´èŒƒå›´: {test_data.index.min()} åˆ° {test_data.index.max()}")
                else:
                    print("âŒ æµ‹è¯•å¤±è´¥")
                
            elif choice == '6':
                print("ğŸ‘‹ å†è§!")
                break
                
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        print(f"âŒ ç¨‹åºé”™è¯¯: {e}")
    finally:
        data_manager.disconnect_mt5()


if __name__ == "__main__":
    main()