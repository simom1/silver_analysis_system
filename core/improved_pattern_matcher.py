"""
æ”¹è¿›ç‰ˆç™½é“¶Kçº¿å½¢æ€ç›¸ä¼¼æ€§åˆ†æå™¨

ä¸»è¦æ”¹è¿›ï¼š
1. æ›´å¥½çš„æ ‡å‡†åŒ–æ–¹æ³•ï¼ˆZ-scoreæ ‡å‡†åŒ–ï¼‰
2. è€ƒè™‘å½¢æ€ç‰¹å¾ï¼ˆæ¶¨è·Œå¹…ã€æ³¢åŠ¨ç‡ã€è¶‹åŠ¿æ–¹å‘ï¼‰
3. æ”¹è¿›çš„ç›¸ä¼¼åº¦è®¡ç®—
4. æ›´åˆç†çš„æƒé‡åˆ†é…
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
import json
import sys
import os
import warnings

# è¿‡æ»¤numpyçš„è­¦å‘Š
warnings.filterwarnings('ignore', category=RuntimeWarning, module='numpy')
warnings.filterwarnings('ignore', message='Degrees of freedom <= 0 for slice')
warnings.filterwarnings('ignore', message='divide by zero encountered')
warnings.filterwarnings('ignore', message='invalid value encountered')

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥æ•°æ®ç®¡ç†å™¨ - æ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—å¯¼å…¥
try:
    from .silver_data_manager import DataManager
except ImportError:
    from silver_data_manager import DataManager

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class PatternMatch:
    """å½¢æ€åŒ¹é…ç»“æœ"""
    symbol: str
    timeframe: str
    similarity_score: float
    match_method: str
    start_index: int
    end_index: int
    start_time: datetime
    end_time: datetime
    pattern_length: int
    trend_similarity: float  # è¶‹åŠ¿ç›¸ä¼¼åº¦
    volatility_similarity: float  # æ³¢åŠ¨ç‡ç›¸ä¼¼åº¦
    shape_similarity: float  # å½¢çŠ¶ç›¸ä¼¼åº¦


class ImprovedPatternMatcher:
    """æ”¹è¿›ç‰ˆç™½é“¶Kçº¿å½¢æ€ç›¸ä¼¼æ€§åˆ†æå™¨"""
    
    def __init__(self, data_dir: str = "market_data"):
        """åˆå§‹åŒ–å½¢æ€åŒ¹é…å™¨"""
        self.data_manager = DataManager(data_dir)
        
        # ç™½é“¶å“ç§ - åŸºå‡†å½¢æ€
        self.silver_symbol = 'XAGUSD'
        self.silver_timeframe = 'H4'
        self.silver_bars = 50  # åŸºå‡†å½¢æ€é•¿åº¦
        
        # è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„å“ç§å’Œæ—¶é—´æ¡†æ¶
        self.target_symbols = self.detect_available_symbols()
    
    def detect_available_symbols(self) -> Dict[str, List[str]]:
        """
        è‡ªåŠ¨æ£€æµ‹æœ¬åœ°å¯ç”¨çš„å“ç§å’Œæ—¶é—´æ¡†æ¶
        
        Returns:
            å¯ç”¨å“ç§å­—å…¸ {symbol: [timeframes]}
        """
        import os
        from pathlib import Path
        
        available_symbols = {}
        
        # æ£€æŸ¥æœ¬åœ°æ•°æ®ç›®å½•
        raw_data_dir = Path(self.data_manager.data_dir) / "raw_data"
        
        if not raw_data_dir.exists():
            logger.warning("æœ¬åœ°æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å“ç§åˆ—è¡¨")
            return self.get_default_symbols()
        
        # æ‰«ææ‰€æœ‰CSVæ–‡ä»¶
        for csv_file in raw_data_dir.glob("*.csv"):
            filename = csv_file.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
            
            # è§£ææ–‡ä»¶å: SYMBOL_TIMEFRAME.csv
            parts = filename.split('_')
            if len(parts) >= 2:
                symbol = '_'.join(parts[:-1])  # å“ç§åï¼ˆå¯èƒ½åŒ…å«ä¸‹åˆ’çº¿ï¼‰
                timeframe = parts[-1]  # æ—¶é—´æ¡†æ¶
                
                # æ’é™¤ç™½é“¶è‡ªå·±
                if symbol == self.silver_symbol:
                    continue
                
                # åªä¿ç•™å¸¸ç”¨æ—¶é—´æ¡†æ¶
                if timeframe in ['H1', 'H4', 'D1']:
                    if symbol not in available_symbols:
                        available_symbols[symbol] = []
                    if timeframe not in available_symbols[symbol]:
                        available_symbols[symbol].append(timeframe)
        
        if not available_symbols:
            logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å“ç§åˆ—è¡¨")
            return self.get_default_symbols()
        
        logger.info(f"æ£€æµ‹åˆ° {len(available_symbols)} ä¸ªå¯ç”¨å“ç§")
        for symbol, timeframes in available_symbols.items():
            logger.info(f"  - {symbol}: {', '.join(timeframes)}")
        
        return available_symbols
    
    def get_default_symbols(self) -> Dict[str, List[str]]:
        """
        è·å–é»˜è®¤å“ç§åˆ—è¡¨ï¼ˆå½“æ— æ³•æ£€æµ‹æœ¬åœ°æ•°æ®æ—¶ä½¿ç”¨ï¼‰
        
        Returns:
            é»˜è®¤å“ç§å­—å…¸
        """
        return {
            'XAUUSD': ['H1', 'H4'],  # é»„é‡‘
            'USOIL': ['H1', 'H4'],   # WTIåŸæ²¹
            'UKOUSD': ['H1', 'H4'],  # å¸ƒä¼¦ç‰¹åŸæ²¹
        }
    
    def normalize_price_series(self, prices: pd.Series) -> np.ndarray:
        """
        ä»·æ ¼åºåˆ—æ ‡å‡†åŒ–
        è½¬æ¢ä¸ºç›¸å¯¹äºç¬¬ä¸€ä¸ªä»·æ ¼çš„ç™¾åˆ†æ¯”å˜åŒ–
        
        Args:
            prices: ä»·æ ¼åºåˆ—
            
        Returns:
            æ ‡å‡†åŒ–åçš„ä»·æ ¼æ•°ç»„
        """
        if len(prices) < 2:
            return np.array([0])
        
        first_price = prices.iloc[0]
        normalized = ((prices - first_price) / first_price * 100).values
        return normalized
    
    def normalize_pattern_zscore(self, prices: pd.Series) -> pd.Series:
        """
        Z-scoreæ ‡å‡†åŒ– - æ›´å¥½åœ°ä¿ç•™å½¢æ€ç‰¹å¾
        
        Args:
            prices: ä»·æ ¼åºåˆ—
            
        Returns:
            æ ‡å‡†åŒ–åçš„ä»·æ ¼åºåˆ—
        """
        if len(prices) < 2:
            return prices
        
        mean = prices.mean()
        std = prices.std()
        
        if std == 0:
            return pd.Series(np.zeros(len(prices)), index=prices.index)
        
        normalized = (prices - mean) / std
        return normalized
    
    def normalize_pattern_minmax(self, prices: pd.Series) -> pd.Series:
        """
        Min-Maxæ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        
        Args:
            prices: ä»·æ ¼åºåˆ—
            
        Returns:
            æ ‡å‡†åŒ–åçš„ä»·æ ¼åºåˆ—
        """
        if len(prices) < 2:
            return prices
        
        min_price = prices.min()
        max_price = prices.max()
        
        if max_price == min_price:
            return pd.Series(np.zeros(len(prices)), index=prices.index)
        
        normalized = (prices - min_price) / (max_price - min_price)
        return normalized
    
    def extract_pattern_features(self, data: pd.DataFrame) -> Dict:
        """
        æå–å½¢æ€ç‰¹å¾
        
        Args:
            data: OHLCæ•°æ®
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        close_prices = data['close']
        high_prices = data['high']
        low_prices = data['low']
        
        # 1. æ€»ä½“æ¶¨è·Œå¹…
        total_return = (close_prices.iloc[-1] - close_prices.iloc[0]) / close_prices.iloc[0]
        
        # 2. æ³¢åŠ¨ç‡ï¼ˆæ ‡å‡†å·®ï¼‰
        returns = close_prices.pct_change().dropna()
        volatility = returns.std()
        
        # 3. æœ€å¤§æ¶¨å¹…å’Œæœ€å¤§è·Œå¹…
        max_gain = (high_prices.max() - close_prices.iloc[0]) / close_prices.iloc[0]
        max_loss = (low_prices.min() - close_prices.iloc[0]) / close_prices.iloc[0]
        
        # 4. è¶‹åŠ¿æ–¹å‘ï¼ˆçº¿æ€§å›å½’æ–œç‡ï¼‰
        x = np.arange(len(close_prices))
        y = close_prices.values
        trend_slope = np.polyfit(x, y, 1)[0]
        
        # 5. è½¬æŠ˜ç‚¹æ•°é‡ï¼ˆä»·æ ¼æ–¹å‘æ”¹å˜çš„æ¬¡æ•°ï¼‰
        price_changes = np.diff(close_prices.values)
        direction_changes = np.sum(np.diff(np.sign(price_changes)) != 0)
        
        # 6. ä¸Šæ¶¨Kçº¿å’Œä¸‹è·ŒKçº¿æ¯”ä¾‹
        shifted_prices = np.roll(close_prices.values, 1)
        up_bars = np.sum(close_prices.values[1:] > shifted_prices[1:])
        down_bars = len(close_prices) - 1 - up_bars
        up_ratio = up_bars / (len(close_prices) - 1) if len(close_prices) > 1 else 0.5
        
        return {
            'total_return': total_return,
            'volatility': volatility,
            'max_gain': max_gain,
            'max_loss': max_loss,
            'trend_slope': trend_slope,
            'direction_changes': direction_changes,
            'up_ratio': up_ratio
        }
    
    def calculate_shape_similarity(self, pattern1: pd.Series, pattern2: pd.Series) -> float:
        """
        è®¡ç®—å½¢çŠ¶ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼‰
        
        Args:
            pattern1: å½¢æ€1ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
            pattern2: å½¢æ€2ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
            
        Returns:
            ç›¸ä¼¼æ€§åˆ†æ•° (0-1)
        """
        if len(pattern1) != len(pattern2) or len(pattern1) < 3:
            return 0.0
        
        # æ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦ä¸º0ï¼ˆé¿å…é™¤é›¶è­¦å‘Šï¼‰
        if pattern1.std() == 0 or pattern2.std() == 0:
            return 0.0
        
        try:
            correlation = pattern1.corr(pattern2)
            
            if pd.isna(correlation) or np.isinf(correlation):
                return 0.0
            
            # ç›¸å…³ç³»æ•°èŒƒå›´ [-1, 1]ï¼Œè½¬æ¢åˆ° [0, 1]
            # æ³¨æ„ï¼šæˆ‘ä»¬åªå…³å¿ƒæ­£ç›¸å…³ï¼ˆå½¢æ€ç›¸ä¼¼ï¼‰ï¼Œè´Ÿç›¸å…³è¡¨ç¤ºåå‘å½¢æ€
            return max(0, correlation)
        except:
            return 0.0
    
    def calculate_trend_similarity(self, features1: Dict, features2: Dict) -> float:
        """
        è®¡ç®—è¶‹åŠ¿ç›¸ä¼¼åº¦
        
        Args:
            features1: å½¢æ€1çš„ç‰¹å¾
            features2: å½¢æ€2çš„ç‰¹å¾
            
        Returns:
            ç›¸ä¼¼æ€§åˆ†æ•° (0-1)
        """
        # 1. æ€»ä½“æ¶¨è·Œå¹…ç›¸ä¼¼åº¦
        return_diff = abs(features1['total_return'] - features2['total_return'])
        return_sim = 1 / (1 + return_diff * 10)  # å·®å¼‚è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
        
        # 2. è¶‹åŠ¿æ–œç‡ç›¸ä¼¼åº¦
        slope_diff = abs(features1['trend_slope'] - features2['trend_slope'])
        slope_sim = 1 / (1 + slope_diff * 100)
        
        # 3. ä¸Šæ¶¨æ¯”ä¾‹ç›¸ä¼¼åº¦
        up_ratio_diff = abs(features1['up_ratio'] - features2['up_ratio'])
        up_ratio_sim = 1 - up_ratio_diff
        
        # ç»¼åˆè¶‹åŠ¿ç›¸ä¼¼åº¦
        trend_similarity = (return_sim * 0.4 + slope_sim * 0.4 + up_ratio_sim * 0.2)
        
        return trend_similarity
    
    def calculate_volatility_similarity(self, features1: Dict, features2: Dict) -> float:
        """
        è®¡ç®—æ³¢åŠ¨ç‡ç›¸ä¼¼åº¦
        
        Args:
            features1: å½¢æ€1çš„ç‰¹å¾
            features2: å½¢æ€2çš„ç‰¹å¾
            
        Returns:
            ç›¸ä¼¼æ€§åˆ†æ•° (0-1)
        """
        # 1. æ³¢åŠ¨ç‡ç›¸ä¼¼åº¦
        vol_diff = abs(features1['volatility'] - features2['volatility'])
        vol_sim = 1 / (1 + vol_diff * 50)
        
        # 2. æœ€å¤§æ¶¨å¹…ç›¸ä¼¼åº¦
        max_gain_diff = abs(features1['max_gain'] - features2['max_gain'])
        max_gain_sim = 1 / (1 + max_gain_diff * 10)
        
        # 3. æœ€å¤§è·Œå¹…ç›¸ä¼¼åº¦
        max_loss_diff = abs(features1['max_loss'] - features2['max_loss'])
        max_loss_sim = 1 / (1 + max_loss_diff * 10)
        
        # 4. è½¬æŠ˜ç‚¹æ•°é‡ç›¸ä¼¼åº¦
        direction_diff = abs(features1['direction_changes'] - features2['direction_changes'])
        direction_sim = 1 / (1 + direction_diff * 0.1)
        
        # ç»¼åˆæ³¢åŠ¨ç‡ç›¸ä¼¼åº¦
        volatility_similarity = (vol_sim * 0.3 + max_gain_sim * 0.25 + 
                                max_loss_sim * 0.25 + direction_sim * 0.2)
        
        return volatility_similarity
    
    def find_similar_patterns(self, target_data: pd.DataFrame, silver_pattern: pd.Series,
                            silver_features: Dict, symbol: str, timeframe: str, 
                            window_size: int = 50) -> List[PatternMatch]:
        """
        åœ¨ç›®æ ‡æ•°æ®ä¸­å¯»æ‰¾ç›¸ä¼¼å½¢æ€
        
        Args:
            target_data: ç›®æ ‡å“ç§æ•°æ®
            silver_pattern: ç™½é“¶åŸºå‡†å½¢æ€ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
            silver_features: ç™½é“¶å½¢æ€ç‰¹å¾
            symbol: å“ç§ä»£ç 
            timeframe: æ—¶é—´æ¡†æ¶
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            
        Returns:
            ç›¸ä¼¼å½¢æ€åŒ¹é…ç»“æœåˆ—è¡¨
        """
        matches = []
        
        if len(target_data) < window_size:
            logger.warning(f"{symbol} {timeframe} æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
            return matches
        
        # æ»‘åŠ¨çª—å£æœç´¢
        # æ­¥é•¿è®¾ç½®ï¼šæ¯æ¬¡ç§»åŠ¨1æ ¹Kçº¿ï¼Œç¡®ä¿ä¸é—æ¼ä»»ä½•å¯èƒ½çš„åŒ¹é…
        # å¦‚æœæ•°æ®é‡å¤ªå¤§å¯ä»¥è®¾ç½®ä¸º3-5ï¼Œä½†ä¼šé™ä½ç²¾åº¦
        step = 1
        total_windows = len(target_data) - window_size + 1
        
        for i in range(0, total_windows, step):
            window_data = target_data.iloc[i:i + window_size]
            
            # æ ‡å‡†åŒ–ä»·æ ¼å½¢æ€
            window_pattern = self.normalize_pattern_zscore(window_data['close'])
            
            # æå–ç‰¹å¾
            window_features = self.extract_pattern_features(window_data)
            
            # è®¡ç®—ä¸‰ç§ç›¸ä¼¼åº¦
            shape_sim = self.calculate_shape_similarity(silver_pattern, window_pattern)
            trend_sim = self.calculate_trend_similarity(silver_features, window_features)
            vol_sim = self.calculate_volatility_similarity(silver_features, window_features)
            
            # ç»¼åˆç›¸ä¼¼æ€§åˆ†æ•°ï¼ˆæ–°çš„æƒé‡åˆ†é…ï¼‰
            weights = {
                'shape': 0.5,      # å½¢çŠ¶æœ€é‡è¦
                'trend': 0.3,      # è¶‹åŠ¿æ¬¡ä¹‹
                'volatility': 0.2  # æ³¢åŠ¨ç‡
            }
            
            combined_score = (
                shape_sim * weights['shape'] +
                trend_sim * weights['trend'] +
                vol_sim * weights['volatility']
            )
            
            # å¦‚æœå½¢çŠ¶ç›¸ä¼¼åº¦å¾ˆä½ï¼Œä½†è¶‹åŠ¿å’Œæ³¢åŠ¨ç›¸ä¼¼ï¼Œä¹Ÿç»™ä¸€å®šåˆ†æ•°
            # è¿™æ ·å¯ä»¥æ‰¾åˆ°"èµ°åŠ¿æ–¹å‘ç›¸ä¼¼"çš„å½¢æ€ï¼Œå³ä½¿å…·ä½“å½¢çŠ¶ä¸å®Œå…¨ä¸€æ ·
            if shape_sim < 0.3 and trend_sim > 0.5 and vol_sim > 0.5:
                # ç»™äºˆè¶‹åŠ¿å’Œæ³¢åŠ¨æ›´é«˜çš„æƒé‡
                alternative_score = trend_sim * 0.5 + vol_sim * 0.5
                combined_score = max(combined_score, alternative_score * 0.8)  # æœ€å¤šç»™åˆ°0.8çš„æƒé‡
            
            # åˆ›å»ºåŒ¹é…ç»“æœ
            match = PatternMatch(
                symbol=symbol,
                timeframe=timeframe,
                similarity_score=combined_score,
                match_method=f"å½¢çŠ¶:{shape_sim:.3f} è¶‹åŠ¿:{trend_sim:.3f} æ³¢åŠ¨:{vol_sim:.3f}",
                start_index=i,
                end_index=i + window_size - 1,
                start_time=window_data.index[0],
                end_time=window_data.index[-1],
                pattern_length=window_size,
                trend_similarity=trend_sim,
                volatility_similarity=vol_sim,
                shape_similarity=shape_sim
            )
            
            matches.append(match)
        
        return matches
    
    def run_pattern_matching(self, top_n: int = 10, min_similarity: float = 0.3, 
                           exclude_recent: bool = True, show_all_top: bool = False) -> List[PatternMatch]:
        """
        è¿è¡Œå½¢æ€åŒ¹é…åˆ†æ
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. è·å–ç™½é“¶(XAGUSD) 4Hå‘¨æœŸçš„æœ€æ–°50æ ¹Kçº¿ä½œä¸ºåŸºå‡†å½¢æ€
        2. åœ¨å…¶ä»–å“ç§çš„å†å²æ•°æ®ä¸­ï¼Œç”¨50æ ¹Kçº¿çš„æ»‘åŠ¨çª—å£æœç´¢ç›¸ä¼¼å½¢æ€
        3. è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„å‰Nä¸ªåŒ¹é…ç»“æœ
        
        Args:
            top_n: è¿”å›å‰Nä¸ªæœ€ç›¸ä¼¼çš„å½¢æ€
            min_similarity: æœ€å°ç›¸ä¼¼æ€§é˜ˆå€¼ï¼ˆå»ºè®®0.3-0.5ï¼Œæ’é™¤åŒæœŸåå»ºè®®0.3ï¼‰
            exclude_recent: æ˜¯å¦æ’é™¤ä¸ç™½é“¶åŒæœŸçš„æ•°æ®ï¼ˆé¿å…æ‰¾åˆ°åŒæ­¥èµ°åŠ¿ï¼‰
            show_all_top: æ˜¯å¦æ˜¾ç¤ºå‰Nä¸ªç»“æœï¼ˆå³ä½¿ä½äºé˜ˆå€¼ï¼‰
            
        Returns:
            å½¢æ€åŒ¹é…ç»“æœåˆ—è¡¨
        """
        logger.info("=" * 80)
        logger.info("å¼€å§‹æ”¹è¿›ç‰ˆç™½é“¶Kçº¿å½¢æ€ç›¸ä¼¼æ€§åˆ†æ...")
        logger.info("=" * 80)
        
        # ========== æ­¥éª¤1: è·å–ç™½é“¶æœ€æ–°50æ ¹4H Kçº¿ä½œä¸ºåŸºå‡† ==========
        logger.info(f"ğŸ“Š æ­¥éª¤1: è·å–ç™½é“¶åŸºå‡†å½¢æ€")
        logger.info(f"   å“ç§: {self.silver_symbol}")
        logger.info(f"   å‘¨æœŸ: {self.silver_timeframe}")
        logger.info(f"   Kçº¿æ•°: æœ€æ–° {self.silver_bars} æ ¹")
        
        silver_data_full = self.data_manager.get_data(
            self.silver_symbol, 
            self.silver_timeframe, 
            count=self.silver_bars
        )
        
        if silver_data_full is None or len(silver_data_full) < self.silver_bars:
            logger.error("âŒ æ— æ³•è·å–ç™½é“¶åŸºå‡†æ•°æ®")
            return []
        
        # é‡è¦ï¼šåªå–æœ€å50æ ¹Kçº¿ä½œä¸ºåŸºå‡†
        silver_data = silver_data_full.iloc[-self.silver_bars:]
        
        # ç¡®è®¤è·å–çš„æ˜¯æœ€æ–°æ•°æ®
        logger.info(f"âœ… æˆåŠŸè·å–ç™½é“¶æ•°æ®")
        logger.info(f"   æ—¶é—´èŒƒå›´: {silver_data.index[0]} åˆ° {silver_data.index[-1]}")
        logger.info(f"   æ•°æ®æ¡æ•°: {len(silver_data)} æ ¹Kçº¿")
        logger.info(f"   æœ€æ–°ä»·æ ¼: {silver_data['close'].iloc[-1]:.2f}")
        
        # æ ‡å‡†åŒ–ç™½é“¶ä»·æ ¼å½¢æ€
        silver_pattern = self.normalize_pattern_zscore(silver_data['close'])
        
        # æå–ç™½é“¶å½¢æ€ç‰¹å¾
        silver_features = self.extract_pattern_features(silver_data)
        
        logger.info(f"   å½¢æ€ç‰¹å¾:")
        logger.info(f"   - æ€»æ¶¨è·Œå¹…: {silver_features['total_return']:.2%}")
        logger.info(f"   - æ³¢åŠ¨ç‡: {silver_features['volatility']:.4f}")
        logger.info(f"   - è¶‹åŠ¿æ–œç‡: {silver_features['trend_slope']:.6f}")
        logger.info(f"   - è½¬æŠ˜ç‚¹æ•°: {silver_features['direction_changes']}")
        logger.info(f"   - ä¸Šæ¶¨Kçº¿æ¯”ä¾‹: {silver_features['up_ratio']:.2%}")
        
        all_matches = []
        
        # ========== æ­¥éª¤2: åœ¨å…¶ä»–å“ç§å†å²æ•°æ®ä¸­æœç´¢ç›¸ä¼¼å½¢æ€ ==========
        logger.info(f"\nğŸ“Š æ­¥éª¤2: åœ¨å…¶ä»–å“ç§å†å²æ•°æ®ä¸­æœç´¢ç›¸ä¼¼å½¢æ€")
        logger.info(f"   æ»‘åŠ¨çª—å£å¤§å°: {self.silver_bars} æ ¹Kçº¿")
        logger.info(f"   æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼: {min_similarity}")
        logger.info("")
        # æœç´¢æ‰€æœ‰ç›®æ ‡å“ç§
        for symbol, timeframes in self.target_symbols.items():
            for timeframe in timeframes:
                try:
                    logger.info(f"ğŸ” æœç´¢ {symbol} {timeframe}...")
                    
                    # è·å–ç›®æ ‡å“ç§çš„å†å²æ•°æ®ï¼ˆè¶³å¤Ÿå¤šçš„æ•°æ®ç”¨äºæ»‘åŠ¨çª—å£æœç´¢ï¼‰
                    target_data = self.data_manager.get_data(symbol, timeframe, count=5000)
                    
                    if target_data is None:
                        logger.warning(f"   âš ï¸  æ— æ³•è·å– {symbol} {timeframe} æ•°æ®")
                        continue
                    
                    logger.info(f"   è·å–åˆ° {len(target_data)} æ ¹Kçº¿æ•°æ®")
                    
                    # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
                    if len(target_data) < 1000:
                        logger.warning(f"   âš ï¸  æ•°æ®é‡ä¸è¶³ ({len(target_data)} < 1000)ï¼Œå»ºè®®æ›´æ–°æ•°æ®")
                        if len(target_data) < 100:
                            logger.warning(f"   âš ï¸  æ•°æ®é‡å¤ªå°‘ï¼Œè·³è¿‡æ­¤å“ç§")
                            continue
                    
                    logger.info(f"   æ—¶é—´èŒƒå›´: {target_data.index[0]} åˆ° {target_data.index[-1]}")
                    
                    # æ’é™¤ä¸ç™½é“¶åŸºå‡†æ—¶é—´æ®µé‡å çš„æ•°æ®
                    silver_start_time = silver_data.index[0]
                    silver_end_time = silver_data.index[-1]
                    
                    # è¿‡æ»¤æ‰ä¸ç™½é“¶æ—¶é—´æ®µé‡å çš„æ•°æ®
                    # åªä¿ç•™ç»“æŸæ—¶é—´æ—©äºç™½é“¶å¼€å§‹æ—¶é—´çš„æ•°æ®
                    original_count = len(target_data)
                    target_data = target_data[target_data.index < silver_start_time]
                    excluded_count = original_count - len(target_data)
                    
                    if len(target_data) < self.silver_bars:
                        logger.warning(f"   âš ï¸  æ’é™¤åŒæœŸæ•°æ®åï¼Œå‰©ä½™æ•°æ®ä¸è¶³ ({len(target_data)} < {self.silver_bars})ï¼Œè·³è¿‡")
                        continue
                    
                    logger.info(f"   âœ… æ’é™¤åŒæœŸæ•°æ® ({silver_start_time} ä¹‹å)ï¼Œæ’é™¤äº† {excluded_count} æ ¹Kçº¿")
                    logger.info(f"   æœç´¢èŒƒå›´: {target_data.index[0]} åˆ° {target_data.index[-1]}")
                    logger.info(f"   å¯æœç´¢æ•°æ®: {len(target_data)} æ ¹Kçº¿")
                    
                    # åœ¨å†å²æ•°æ®ä¸­ç”¨50æ ¹Kçº¿çš„æ»‘åŠ¨çª—å£æœç´¢ç›¸ä¼¼å½¢æ€
                    matches = self.find_similar_patterns(
                        target_data, 
                        silver_pattern, 
                        silver_features,
                        symbol, 
                        timeframe, 
                        window_size=self.silver_bars  # å›ºå®šä½¿ç”¨50æ ¹Kçº¿çª—å£
                    )
                    
                    # æ˜¾ç¤ºæ‰€æœ‰åŒ¹é…çš„æœ€é«˜ç›¸ä¼¼åº¦å’Œè¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    if matches:
                        best_match = max(matches, key=lambda x: x.similarity_score)
                        logger.info(f"   ğŸ“Š æœ€é«˜ç›¸ä¼¼åº¦: {best_match.similarity_score:.3f}")
                        logger.info(f"      - å½¢çŠ¶: {best_match.shape_similarity:.3f}")
                        logger.info(f"      - è¶‹åŠ¿: {best_match.trend_similarity:.3f}")
                        logger.info(f"      - æ³¢åŠ¨: {best_match.volatility_similarity:.3f}")
                    
                    # è¿‡æ»¤ä½ç›¸ä¼¼æ€§ç»“æœ
                    filtered_matches = [m for m in matches if m.similarity_score >= min_similarity]
                    
                    if filtered_matches:
                        logger.info(f"   âœ… æ‰¾åˆ° {len(filtered_matches)} ä¸ªç›¸ä¼¼å½¢æ€ (>= {min_similarity})")
                    else:
                        if matches:
                            logger.info(f"   â„¹ï¸  æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ >= {min_similarity} çš„å½¢æ€")
                        else:
                            logger.info(f"   â„¹ï¸  æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…")
                    
                    all_matches.extend(filtered_matches)
                    
                except Exception as e:
                    logger.error(f"   âŒ æœç´¢ {symbol} {timeframe} æ—¶å‡ºé”™: {e}")
                    continue
        
        # ========== æ­¥éª¤3: æ’åºå¹¶è¿”å›ç»“æœ ==========
        logger.info(f"\nğŸ“Š æ­¥éª¤3: æ±‡æ€»ç»“æœ")
        logger.info(f"   æ€»å…±æ‰¾åˆ° {len(all_matches)} ä¸ªç›¸ä¼¼å½¢æ€ (>= {min_similarity})")
        
        # æŒ‰ç›¸ä¼¼æ€§åˆ†æ•°æ’åº
        all_matches.sort(key=lambda x: x.similarity_score, reverse=True)
        
        if all_matches:
            logger.info(f"   æœ€é«˜ç›¸ä¼¼åº¦: {all_matches[0].similarity_score:.3f} ({all_matches[0].symbol} {all_matches[0].timeframe})")
            logger.info(f"   è¿”å›å‰ {min(top_n, len(all_matches))} ä¸ªç»“æœ")
        else:
            logger.warning(f"   âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼åº¦ >= {min_similarity} çš„å½¢æ€")
            if show_all_top:
                logger.info(f"   ğŸ’¡ æç¤º: å¯ä»¥å°è¯•é™ä½é˜ˆå€¼æˆ–æŸ¥çœ‹æ‰€æœ‰ç»“æœ")
        
        logger.info("=" * 80)
        logger.info("âœ… å½¢æ€åŒ¹é…åˆ†æå®Œæˆ")
        logger.info("=" * 80)
        
        return all_matches[:top_n]
    
    def print_pattern_results(self, matches: List[PatternMatch]):
        """æ‰“å°å½¢æ€åŒ¹é…ç»“æœ"""
        if not matches:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„Kçº¿å½¢æ€")
            return
        
        print(f"\n{'='*100}")
        print(f"ğŸ¥ˆ æ”¹è¿›ç‰ˆç™½é“¶Kçº¿å½¢æ€ç›¸ä¼¼æ€§åˆ†æç»“æœ")
        print(f"{'='*100}")
        print(f"åŸºå‡†å½¢æ€: {self.silver_symbol} {self.silver_timeframe} æœ€å{self.silver_bars}æ ¹Kçº¿")
        print(f"æœç´¢ç»“æœ: æ‰¾åˆ° {len(matches)} ä¸ªæœ€ç›¸ä¼¼çš„å½¢æ€")
        print(f"{'='*100}")
        
        print(f"\nğŸ“Š æœ€ç›¸ä¼¼çš„Kçº¿å½¢æ€:")
        print("-" * 100)
        print(f"{'æ’å':<4} {'å“ç§':<8} {'æ—¶é—´æ¡†æ¶':<8} {'ç»¼åˆç›¸ä¼¼åº¦':<10} {'å½¢çŠ¶':<8} {'è¶‹åŠ¿':<8} {'æ³¢åŠ¨':<8} {'æ—¶é—´æ®µ':<32}")
        print("-" * 100)
        
        for i, match in enumerate(matches, 1):
            # ç›¸ä¼¼åº¦ç­‰çº§
            if match.similarity_score >= 0.8:
                level = "ğŸ”´"
            elif match.similarity_score >= 0.7:
                level = "ğŸŸ "
            elif match.similarity_score >= 0.6:
                level = "ğŸŸ¡"
            else:
                level = "ğŸŸ¢"
            
            time_range = f"{match.start_time.strftime('%m-%d %H:%M')} ~ {match.end_time.strftime('%m-%d %H:%M')}"
            
            print(f"{i:<4} {match.symbol:<8} {match.timeframe:<8} "
                  f"{level}{match.similarity_score:<9.3f} "
                  f"{match.shape_similarity:<8.3f} {match.trend_similarity:<8.3f} "
                  f"{match.volatility_similarity:<8.3f} {time_range:<32}")
        
        # æ˜¾ç¤ºæœ€ä½³åŒ¹é…è¯¦æƒ…
        if matches:
            best = matches[0]
            print(f"\nğŸ¯ æœ€ç›¸ä¼¼å½¢æ€è¯¦æƒ…:")
            print(f"   å“ç§: {best.symbol} ({best.timeframe})")
            print(f"   ç»¼åˆç›¸ä¼¼åº¦: {best.similarity_score:.4f}")
            print(f"   - å½¢çŠ¶ç›¸ä¼¼åº¦: {best.shape_similarity:.4f}")
            print(f"   - è¶‹åŠ¿ç›¸ä¼¼åº¦: {best.trend_similarity:.4f}")
            print(f"   - æ³¢åŠ¨ç›¸ä¼¼åº¦: {best.volatility_similarity:.4f}")
            print(f"   æ—¶é—´æ®µ: {best.start_time} åˆ° {best.end_time}")
            print(f"   å½¢æ€é•¿åº¦: {best.pattern_length} æ ¹Kçº¿")
            
            if best.similarity_score >= 0.7:
                print(f"\nğŸ’¡ å½¢æ€åˆ†æ:")
                print(f"   â€¢ è¯¥æ—¶é—´æ®µçš„ {best.symbol} èµ°åŠ¿ä¸å½“å‰ç™½é“¶å½¢æ€é«˜åº¦ç›¸ä¼¼")
                print(f"   â€¢ å¯ä»¥å‚è€ƒè¯¥æ—¶é—´æ®µåç»­çš„ä»·æ ¼èµ°åŠ¿")
                print(f"   â€¢ å»ºè®®ç»“åˆå½“æ—¶çš„å¸‚åœºç¯å¢ƒè¿›è¡Œåˆ†æ")
            else:
                print(f"\nâš ï¸  æ³¨æ„: ç›¸ä¼¼åº¦ä¸­ç­‰ï¼Œå»ºè®®è°¨æ…å‚è€ƒ")
    
    def save_pattern_results(self, matches: List[PatternMatch], filename: Optional[str] = None):
        """ä¿å­˜å½¢æ€åŒ¹é…ç»“æœ"""
        # ç¡®ä¿ outputs ç›®å½•å­˜åœ¨
        output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'outputs')
        os.makedirs(output_dir, exist_ok=True)
        
        if not filename:
            filename = f"improved_pattern_matches_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # ä½¿ç”¨å®Œæ•´è·¯å¾„
        filepath = os.path.join(output_dir, filename)
        
        try:
            data = {
                "analysis_info": {
                    "timestamp": datetime.now().isoformat(),
                    "silver_symbol": self.silver_symbol,
                    "silver_timeframe": self.silver_timeframe,
                    "silver_bars": self.silver_bars,
                    "total_matches": len(matches),
                    "analysis_type": "improved_pattern_similarity",
                    "algorithm_version": "2.0"
                },
                "matches": []
            }
            
            for match in matches:
                data["matches"].append({
                    "symbol": match.symbol,
                    "timeframe": match.timeframe,
                    "similarity_score": match.similarity_score,
                    "shape_similarity": match.shape_similarity,
                    "trend_similarity": match.trend_similarity,
                    "volatility_similarity": match.volatility_similarity,
                    "match_method": match.match_method,
                    "start_time": match.start_time.isoformat(),
                    "end_time": match.end_time.isoformat(),
                    "pattern_length": match.pattern_length
                })
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"å½¢æ€åŒ¹é…ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ”¹è¿›ç‰ˆç™½é“¶Kçº¿å½¢æ€ç›¸ä¼¼æ€§åˆ†æå™¨")
    print("=" * 60)
    print("æ”¹è¿›ç‚¹:")
    print("1. ä½¿ç”¨Z-scoreæ ‡å‡†åŒ–ï¼Œæ›´å¥½åœ°ä¿ç•™å½¢æ€ç‰¹å¾")
    print("2. æå–å¤šç»´åº¦ç‰¹å¾ï¼ˆæ¶¨è·Œå¹…ã€æ³¢åŠ¨ç‡ã€è¶‹åŠ¿ã€è½¬æŠ˜ç‚¹ï¼‰")
    print("3. åˆ†åˆ«è®¡ç®—å½¢çŠ¶ã€è¶‹åŠ¿ã€æ³¢åŠ¨ç‡ç›¸ä¼¼åº¦")
    print("4. æ›´åˆç†çš„æƒé‡åˆ†é…å’Œé˜ˆå€¼è®¾ç½®")
    print("=" * 60)
    
    # åˆ›å»ºå½¢æ€åŒ¹é…å™¨
    matcher = ImprovedPatternMatcher()
    
    try:
        while True:
            print(f"\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. è¿è¡Œæ”¹è¿›ç‰ˆå½¢æ€åŒ¹é…åˆ†æ")
            print("2. æŸ¥çœ‹æœ¬åœ°æ•°æ®çŠ¶æ€")
            print("3. æ›´æ–°æ•°æ®")
            print("4. é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
            
            if choice == '1':
                print("\nğŸ” å¼€å§‹æ”¹è¿›ç‰ˆå½¢æ€åŒ¹é…åˆ†æ...")
                
                # è®¾ç½®å‚æ•°
                print("\n" + "=" * 60)
                print("ğŸ“‹ å‚æ•°è¯´æ˜")
                print("=" * 60)
                print("1ï¸âƒ£  è¿”å›ç»“æœæ•°:")
                print("   - æ‰¾åˆ°å¤šå°‘ä¸ªç›¸ä¼¼çš„å†å²å½¢æ€")
                print("   - æ¯ä¸ªå½¢æ€ = 50æ ¹è¿ç»­Kçº¿")
                print("   - ä¾‹å¦‚: è¾“å…¥10 = æ‰¾10ä¸ªå†å²å½¢æ€")
                print()
                print("2ï¸âƒ£  ç›¸ä¼¼åº¦é˜ˆå€¼:")
                print("   - è¿‡æ»¤æ‰ç›¸ä¼¼åº¦ä½çš„å½¢æ€")
                print("   - èŒƒå›´: 0.0 - 1.0 (è¶Šé«˜è¶Šç›¸ä¼¼)")
                print("   - æ¨è: 0.3-0.5 (æ’é™¤åŒæœŸæ•°æ®åå»ºè®®é™ä½)")
                print()
                print("âš ï¸  æ³¨æ„: ç³»ç»Ÿä¼šè‡ªåŠ¨æ’é™¤ä¸ç™½é“¶åŒæœŸçš„æ•°æ®")
                print("   è¿™æ ·æ‰¾åˆ°çš„éƒ½æ˜¯å†å²å½¢æ€ï¼Œæœ‰é¢„æµ‹ä»·å€¼")
                print("   ä½†å¯èƒ½å¯¼è‡´ç›¸ä¼¼åº¦æ•´ä½“åä½ï¼Œå»ºè®®é˜ˆå€¼è®¾ä¸º0.3")
                print("=" * 60)
                print()
                
                top_n = int(input("è¿”å›å¤šå°‘ä¸ªç›¸ä¼¼å½¢æ€ (é»˜è®¤10): ") or "10")
                min_similarity = float(input("æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤0.3ï¼Œå»ºè®®0.3-0.5): ") or "0.3")
                
                # è¿è¡Œåˆ†æ
                matches = matcher.run_pattern_matching(top_n=top_n, min_similarity=min_similarity)
                
                # æ˜¾ç¤ºç»“æœ
                matcher.print_pattern_results(matches)
                
                # ä¿å­˜ç»“æœ
                if matches:
                    save = input("\næ˜¯å¦ä¿å­˜ç»“æœ? (y/N): ").strip().lower()
                    if save in ['y', 'yes', 'æ˜¯']:
                        matcher.save_pattern_results(matches)
                
            elif choice == '2':
                print("\nğŸ“Š æœ¬åœ°æ•°æ®çŠ¶æ€:")
                matcher.data_manager.print_data_summary()
                
            elif choice == '3':
                print("\nğŸ”„ æ›´æ–°æ•°æ®...")
                all_symbols = dict(matcher.target_symbols)
                all_symbols[matcher.silver_symbol] = [matcher.silver_timeframe]
                
                results = matcher.data_manager.batch_update_data(all_symbols, count=5000)
                
                print(f"\nğŸ“Š æ›´æ–°ç»“æœ:")
                for symbol, symbol_results in results.items():
                    for timeframe, success in symbol_results.items():
                        status = "âœ…" if success else "âŒ"
                        print(f"{status} {symbol} {timeframe}")
                
            elif choice == '4':
                print("ğŸ‘‹ å†è§!")
                break
                
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        print(f"âŒ ç¨‹åºé”™è¯¯: {e}")
    finally:
        matcher.data_manager.disconnect_mt5()


if __name__ == "__main__":
    main()
